"""ë‹¤ìš´ë¡œë“œ API ì—”ë“œí¬ì¸íŠ¸"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, HttpUrl
from typing import List, Optional, Dict, Any
import json
import time
import uuid
from loguru import logger

# Celery / workers
from ..workers.tasks import batch_download_task, get_task_status, get_batch_status
from ..workers.celery_app import celery_app

# Redis/core helpers
from ..core.redis_client import (
    save_batch,
    get_batch as redis_get_batch,
    redis_client,
    publish_event,
    get_batch_key,
)
from ..core.config import settings

# For async Redis Pub/Sub
import redis.asyncio as aioredis

# DownloadItem and DownloadBatch models are now in models/download.py
from ..models.download import DownloadItem, DownloadBatch

router = APIRouter(prefix="/download", tags=["Download"])


class DownloadRequest(BaseModel):
    urls: List[HttpUrl]
    batch_name: Optional[str] = None
    use_celery: bool = True  # Celery ì‚¬ìš© ì—¬ë¶€


# DownloadItem and DownloadBatch models are now in models/download.py

# ----------------------------------------------------------------------------
# NOTE: ê¸°ì¡´ in-memory dict â†’ Redis ì „í™˜
# ----------------------------------------------------------------------------
# download_batches: Dict[str, DownloadBatch] = {}

# Helper to list all batches from Redis

def list_all_batches(status_filter: Optional[str] = None) -> List[Dict[str, Any]]:  # íƒ€ì… ìˆ˜ì •
    """
    Redisì—ì„œ ëª¨ë“  ë°°ì¹˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    status_filterë¥¼ ì‚¬ìš©í•˜ì—¬ 'active', 'pending' ë“±ì˜ ìƒíƒœë¡œ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” list_all_batches í˜¸ì¶œë¨: status_filter={status_filter}")
    
    batches: List[Dict[str, Any]] = []
    batch_count = 0
    
    # Redis í‚¤ í™•ì¸
    redis_keys = list(redis_client.scan_iter("batch:*"))
    logger.info(f"ğŸ“¦ Redisì—ì„œ ë°œê²¬ëœ ë°°ì¹˜ í‚¤: {len(redis_keys)}ê°œ - {redis_keys}")
    
    for key in redis_keys:
        batch_count += 1
        logger.debug(f"ğŸ”„ ë°°ì¹˜ í‚¤ ì²˜ë¦¬ ì¤‘ ({batch_count}/{len(redis_keys)}): {key}")
        
        raw = redis_client.get(key)
        if not raw:
            logger.warning(f"âŒ ë¹ˆ ë°°ì¹˜ ë°ì´í„°: {key}")
            continue
            
        try:
            data: Dict[str, Any] = json.loads(raw)
            batch_id = data.get('id', 'unknown')
            batch_status = data.get('status', 'unknown')
            
            logger.debug(f"ğŸ“‹ ë°°ì¹˜ ë°ì´í„° íŒŒì‹± ì„±ê³µ: {batch_id} - status: {batch_status}")

            # ë°ì´í„° í´ë Œì§•: items ë‚´ë¶€ì˜ None ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜
            items = data.get("items", [])
            logger.debug(f"ğŸ“ ë°°ì¹˜ {batch_id}ì˜ ì•„ì´í…œ ê°œìˆ˜: {len(items)}")
            
            for item_data in items:
                if item_data.get("total_size_bytes") is None:
                    item_data["total_size_bytes"] = 0
                if item_data.get("downloaded_bytes") is None:
                    item_data["downloaded_bytes"] = 0

            # --- reconcile aggregate counts for legacy or partially updated batches ---
            total_items = len(items)
            completed_items = sum(1 for it in items if it.get("status") == "completed")
            failed_items = sum(1 for it in items if it.get("status") == "failed")
            
            data["total_items"] = total_items
            data["completed_items"] = completed_items
            data["failed_items"] = failed_items
            
            logger.debug(f"ğŸ“Š ë°°ì¹˜ {batch_id} í†µê³„: ì „ì²´={total_items}, ì™„ë£Œ={completed_items}, ì‹¤íŒ¨={failed_items}")
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ ë¡œì§
            original_status = data.get("status")
            if total_items > 0:
                if completed_items == total_items:
                    data["status"] = "completed"
                    data.setdefault("completed_at", time.time())
                elif failed_items == total_items:
                    data["status"] = "failed"
                    data.setdefault("completed_at", time.time())
                elif completed_items + failed_items == total_items:
                    data["status"] = "completed_with_errors"
                    data.setdefault("completed_at", time.time())
                elif any(it.get("status") == "downloading" for it in items):
                     data["status"] = "downloading"
            
            if original_status != data["status"]:
                logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_id} ìƒíƒœ ë³€ê²½: {original_status} â†’ {data['status']}")
            
            # ìƒíƒœ í•„í„° ì ìš©
            if status_filter:
                # 'active'ëŠ” 'downloading'ê³¼ 'pending'ì„ ëª¨ë‘ í¬í•¨í•˜ë„ë¡ ì²˜ë¦¬
                allowed_statuses = [s.strip() for s in status_filter.split(',')]
                if "active" in allowed_statuses:
                    allowed_statuses.extend(["downloading", "pending"])
                
                logger.debug(f"ğŸ¯ í•„í„°ë§ ì²´í¬: í—ˆìš©ëœìƒíƒœ={allowed_statuses}, ë°°ì¹˜ìƒíƒœ={data.get('status')}")
                
                if data.get("status") not in allowed_statuses:
                    logger.debug(f"ğŸš« ë°°ì¹˜ {batch_id} í•„í„°ë§ë¨ (ìƒíƒœ: {data.get('status')})")
                    continue
                else:
                    logger.debug(f"âœ… ë°°ì¹˜ {batch_id} í•„í„° í†µê³¼")

            batches.append(data)
            logger.debug(f"â• ë°°ì¹˜ {batch_id} ê²°ê³¼ì— ì¶”ê°€ë¨")

        except json.JSONDecodeError as e:
            logger.error(f"ğŸ’¥ ë°°ì¹˜ JSON íŒŒì‹± ì‹¤íŒ¨ {key}: {e}")
            continue
        except Exception as exc:
            logger.error(f"ğŸ’¥ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ {key}: {exc}", exc_info=True)
            continue
    
    # ê²°ê³¼ ì •ë ¬
    sorted_batches = sorted(batches, key=lambda b: b.get("created_at", 0), reverse=True)
    
    logger.info(f"ğŸ‰ ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: ì „ì²´ {batch_count}ê°œ ì¤‘ {len(sorted_batches)}ê°œ ë°˜í™˜")
    
    # ë°˜í™˜ë˜ëŠ” ë°°ì¹˜ë“¤ì˜ ìš”ì•½ ë¡œê·¸
    if sorted_batches:
        logger.info("ğŸ“‹ ë°˜í™˜ë˜ëŠ” ë°°ì¹˜ ëª©ë¡:")
        for batch in sorted_batches:
            batch_id = batch.get('id', 'unknown')
            batch_status = batch.get('status', 'unknown') 
            total_items = batch.get('total_items', 0)
            completed_items = batch.get('completed_items', 0)
            logger.info(f"  - {batch_id}: {batch_status} ({completed_items}/{total_items})")
    else:
        logger.warning("âš ï¸ ë°˜í™˜í•  ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    return sorted_batches


@router.post("/start", status_code=201)
async def start_download(
    download_request: DownloadRequest, background_tasks: BackgroundTasks
):
    urls = [str(url) for url in download_request.urls]
    # ì¶”ê°€: URL ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ë¦¬
    cleaned_urls = []
    for url in urls:
        # ì—¬ëŸ¬ URLì´ ì—°ê²°ë˜ì–´ ìˆì„ ê²½ìš° ë¶„ë¦¬ ì‹œë„
        if 'https://' in url and url.count('https://') > 1:
            # ì—¬ëŸ¬ URLì´ ì—°ê²°ëœ ê²½ìš° ë¶„ë¦¬
            parts = url.split('https://')
            for part in parts:
                if part.strip():
                    cleaned_url = 'https://' + part.strip()
                    if cleaned_url.startswith('https://rg.to/') or cleaned_url.startswith('https://rapidgator.net/'):
                        cleaned_urls.append(cleaned_url)
        else:
            if url.strip():
                cleaned_urls.append(url.strip())

    urls = cleaned_urls
    logger.info(f"ì •ë¦¬ëœ URL ê°œìˆ˜: {len(urls)}")
    logger.info(f"ìƒˆë¡œìš´ ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {len(urls)}ê°œ URL")

    batch_id = f"batch:{uuid.uuid4()}"
    items = []
    for url in urls:
        item_id = str(uuid.uuid4())
        filename = url.split("/")[-1].split("?")[0] or f"download_{item_id}"
        items.append(
            DownloadItem(id=item_id, url=url, filename=filename)
        )

    batch = DownloadBatch(
        id=batch_id,
        name=f"Batch {time.strftime('%Y-%m-%d %H:%M')}",
        items=items,
        total_items=len(items),
        use_celery=settings.CELERY_ENABLED,
    )

    # Initial progress calculation
    batch.update_progress()
    
    # Save the initial batch state
    save_batch(batch.model_dump())

    try:
        # If celery is enabled, dispatch the batch download task to the background
        if settings.CELERY_ENABLED:
            logger.info(f"Celery í™œì„±í™”ë¨. {batch.id}ì— ëŒ€í•œ íƒœìŠ¤í¬ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.")
            try:
                # Pass the entire batch data as a dictionary
                batch_download_task.delay(batch.model_dump())
            except Exception as e:
                logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                raise HTTPException(status_code=500, detail=str(e))
        else:
            logger.info(f"Celery ë¹„í™œì„±í™”ë¨. {batch_id}ë¥¼ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            background_tasks.add_task(batch_download_task, batch_id, urls)

    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        batch_data = batch.model_dump()
        batch_data["status"] = "failed"
        batch_data["error_message"] = f"Failed to start download task: {e}"
        save_batch(batch_data)
        raise HTTPException(status_code=500, detail=str(e))

    return batch.model_dump()


@router.get("/batches/{batch_id}", response_model=DownloadBatch)
async def get_batch_status(batch_id: str):
    """ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ìƒíƒœ ì¡°íšŒ"""
    data = redis_get_batch(batch_id)
    if not data:
        raise HTTPException(status_code=404, detail="Batch not found")

    batch = DownloadBatch.model_validate(data)
    
    if batch.use_celery:
        # Celery íƒœìŠ¤í¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        await update_batch_from_celery(batch)
    
    return batch


@router.delete("/batches")
async def clear_all_batches():
    """ëª¨ë“  ë°°ì¹˜ í‚¤ë¥¼ Redisì—ì„œ ì œê±° (ê´€ë¦¬ìš©)"""
    keys = list(redis_client.scan_iter("batch:*"))
    deleted = 0
    if keys:
        deleted = redis_client.delete(*keys)
    # broadcast to SSE listeners that everything cleared
    try:
        publish_event("batch:all:events", {"type": "all_cleared"})
    except Exception:
        pass
    return {"deleted_batches": deleted}


@router.get("/batches")
async def list_batches(status: Optional[str] = None):
    """
    ëª¨ë“  ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ. status ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ í•„í„°ë§ ê°€ëŠ¥.
    ì˜ˆ: /batches?status=active
    """
    logger.info(f"ğŸŒ API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ: /batches?status={status}")
    
    try:
        result = list_all_batches(status_filter=status)
        logger.info(f"âœ… API ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ: {len(result)}ê°œ ë°°ì¹˜")
        return result
    except Exception as e:
        logger.error(f"ğŸ’¥ ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# ë””ë²„ê¹…ìš© ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.get("/debug/batches")
async def debug_batches(status: Optional[str] = None):
    """ğŸ”§ ë””ë²„ê¹…ìš© ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ"""
    logger.info(f"ğŸ› ï¸ ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ: status={status}")
    
    try:
        # Redis ì—°ê²° í™•ì¸
        redis_info = redis_client.info()
        logger.info(f"ğŸ”— Redis ì—°ê²° ìƒíƒœ: {redis_info.get('connected_clients', 'unknown')} í´ë¼ì´ì–¸íŠ¸")
        
        # Redis í‚¤ í™•ì¸
        keys = list(redis_client.scan_iter("batch:*"))
        logger.info(f"ğŸ”‘ Redis í‚¤ ê°œìˆ˜: {len(keys)}")
        
        # ê° í‚¤ì˜ ë‚´ìš© í™•ì¸
        raw_batches = []
        for key in keys:
            raw = redis_client.get(key)
            if raw:
                try:
                    data = json.loads(raw)
                    raw_batches.append({
                        "key": key,
                        "id": data.get("id"),
                        "status": data.get("status"),
                        "total_items": data.get("total_items", 0),
                        "completed_items": data.get("completed_items", 0)
                    })
                except:
                    raw_batches.append({"key": key, "error": "JSON íŒŒì‹± ì‹¤íŒ¨"})
        
        # list_all_batches í˜¸ì¶œ
        filtered_batches = list_all_batches(status_filter=status)
        
        return {
            "ğŸ”§ debug_info": {
                "status_filter": status,
                "redis_connected": True,
                "redis_keys_count": len(keys),
                "filtered_batches_count": len(filtered_batches)
            },
            "ğŸ”‘ redis_keys": keys,
            "ğŸ“¦ raw_batches": raw_batches,
            "ğŸ¯ filtered_batches": filtered_batches
        }
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "error": str(e),
            "redis_connected": False
        }


# -----------------------------------------------------------------------------
# Retry failed items endpoint
# -----------------------------------------------------------------------------


@router.post("/batches/{batch_id}/retry-failed")
async def retry_failed_items(batch_id: str):
    """Requeue all failed items in a batch and return updated batch info."""
    data = redis_get_batch(batch_id)
    if not data:
        raise HTTPException(status_code=404, detail="Batch not found")

    batch = DownloadBatch.model_validate(data)

    retry_count = 0
    for item in batch.items:
        if item.status == "failed":
            # requeue
            task_result = batch_download_task.app.send_task(
                "download_file",
                kwargs={
                    "url": item.url,
                    "batch_id": batch_id,
                    "item_id": item.id,
                    "download_dir": settings.download_path,
                },
            )
            item.task_id = task_result.id
            item.status = "queued"
            retry_count += 1

    if retry_count == 0:
        return {"message": "No failed items to retry", "batch": batch}

    # save and publish event
    save_batch(batch.model_dump(mode="json", by_alias=False))
    publish_event(f"batch:{batch_id}:events", {"type": "retry_started", "count": retry_count})

    return {"message": f"Requeued {retry_count} items", "batch": batch}


# -----------------------------------------------------------------------------
# SSE streaming endpoint
# -----------------------------------------------------------------------------


@router.get("/stream/{batch_id}")
async def stream_batch_updates(batch_id: str):
    """Server-Sent Events endpoint streaming Redis Pub/Sub messages for batch."""

    channel = f"batch:{batch_id}:events"

    async def event_generator():
        redis_async = aioredis.from_url(settings.redis_url, decode_responses=True)
        pubsub = redis_async.pubsub()
        await pubsub.subscribe(channel)
        try:
            # send initial snapshot
            snapshot = redis_get_batch(batch_id)
            if snapshot:
                yield f"data: {json.dumps({'type': 'snapshot', 'data': snapshot})}\n\n"

            async for message in pubsub.listen():
                if message["type"] != "message":
                    continue
                payload = message["data"]
                yield f"data: {payload}\n\n"
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.close()

    return StreamingResponse(event_generator(), media_type="text/event-stream")


# -----------------------------------------------------------------------------
# Global SSE stream for all batches
# -----------------------------------------------------------------------------


@router.get("/stream")
async def stream_all_batches():
    """SSE endpoint that streams snapshots/updates for *all* batches."""

    async def event_generator():
        r = aioredis.from_url(settings.redis_url, decode_responses=True)
        pubsub = r.pubsub()
        channel = "batch:all:events"
        await pubsub.subscribe(channel)

        # Initial: send list of current batches for initial render
        try:
            keys = await r.keys("batch:*")  # may include non-batch keys but fine
            snapshots = []
            if keys:
                raw_vals = await r.mget(keys)
                for raw in raw_vals:
                    if raw:
                        try:
                            snapshots.append(json.loads(raw))
                        except Exception:
                            pass
            yield f"data: {json.dumps({'type':'init', 'data': snapshots})}\n\n"

            async for message in pubsub.listen():
                if message["type"] != "message":
                    continue
                yield f"data: {message['data']}\n\n"
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.close()

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@router.delete("/batches/{batch_id}")
async def delete_batch(batch_id: str):
    """ë°°ì¹˜ ì‚­ì œ"""
    data = redis_get_batch(batch_id)
    if not data:
        raise HTTPException(status_code=404, detail="Batch not found")

    batch = DownloadBatch.model_validate(data)
    
    # Celery íƒœìŠ¤í¬ ì·¨ì†Œ (ì‹œë„)
    if batch.use_celery:
        try:
            # ê° ì•„ì´í…œì˜ íƒœìŠ¤í¬ ì·¨ì†Œ
            for item in batch.items:
                if item.task_id:
                    celery_app.control.revoke(item.task_id, terminate=True)
            
            logger.info(f"Celery íƒœìŠ¤í¬ë“¤ ì·¨ì†Œë¨: {batch_id}")
        except Exception as e:
            logger.warning(f"Celery íƒœìŠ¤í¬ ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}")
    
    # ë°°ì¹˜ ì‚­ì œ
    redis_client.delete(f"batch:{batch_id}")
    
    return {"message": f"Batch {batch_id} deleted"}


@router.get("/tasks/{task_id}")
async def get_task_status_endpoint(task_id: str):
    """ê°œë³„ Celery íƒœìŠ¤í¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        status = get_task_status(task_id)
        return status
    except Exception as e:
        logger.error(f"íƒœìŠ¤í¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {task_id} - {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get task status: {str(e)}")


# Redisì— ìƒíƒœë¥¼ ë°˜ì˜í•˜ë„ë¡ í…œí¬ëŸ¬ë¦¬ ìˆ˜ì • (ì¶”í›„ Pub/Sub í†µí•© ì‹œ ê°œì„ )
async def update_batch_from_celery(batch: DownloadBatch):
    """Celeryì—ì„œ ë°°ì¹˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸"""
    try:
        # ë°°ì¹˜ì˜ ê° ì•„ì´í…œì— ëŒ€í•´ Celery íƒœìŠ¤í¬ ìƒíƒœ í™•ì¸
        # ì‹¤ì œë¡œëŠ” ë°°ì¹˜ íƒœìŠ¤í¬ì—ì„œ ê°œë³„ íƒœìŠ¤í¬ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
        
        # ë°°ì¹˜ íƒœìŠ¤í¬ IDë¥¼ ì €ì¥í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, ê° ì•„ì´í…œì˜ íƒœìŠ¤í¬ IDë¥¼ ì°¾ì•„ì•¼ í•¨
        # ì´ëŠ” ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°°ì¹˜ íƒœìŠ¤í¬ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜´
        
        completed_count = 0
        failed_count = 0
        in_progress_count = 0
        
        for item in batch.items:
            if item.task_id:
                task_status = get_task_status(item.task_id)
                
                if task_status["status"] == "SUCCESS":
                    item.status = "completed"
                    item.progress = 100.0
                    
                    # ê²°ê³¼ì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    if task_status.get("result"):
                        result = task_status["result"]
                        if isinstance(result, dict):
                            item.filename = result.get("filename")
                            item.file_size = result.get("file_size")
                    
                    completed_count += 1
                    
                elif task_status["status"] == "FAILURE":
                    item.status = "failed"
                    item.error_message = str(task_status.get("result", "Unknown error"))
                    failed_count += 1
                    
                elif task_status["status"] in ["PROGRESS", "RETRY"]:
                    item.status = "downloading"
                    
                    # ì§„í–‰ë¥  ì •ë³´ ì¶”ì¶œ
                    if task_status.get("info"):
                        info = task_status["info"]
                        if isinstance(info, dict):
                            current = info.get("current", 0)
                            total = info.get("total", 100)
                            item.progress = (current / total) * 100 if total > 0 else 0
                    
                    in_progress_count += 1
                    
                else:
                    item.status = "pending"
        
        # ë°°ì¹˜ ì „ì²´ ìƒíƒœ ì—…ë°ì´íŠ¸
        batch.completed_items = completed_count
        batch.failed_items = failed_count
        
        total_items = len(batch.items)
        if completed_count == total_items:
            batch.status = "completed"
            batch.completed_at = time.time()
        elif failed_count == total_items:
            batch.status = "failed"
            batch.completed_at = time.time()
        elif completed_count + failed_count == total_items:
            batch.status = "completed_with_errors"
            batch.completed_at = time.time()
        elif in_progress_count > 0:
            batch.status = "downloading"
        else:
            batch.status = "pending"
            
        # Redisì— ì €ì¥ (ìƒíƒœ ìŠ¤ëƒ…ìƒ·)
        save_batch(batch.model_dump(mode="json", by_alias=False))
            
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {batch.id} - {str(e)}")


# ê¸°ì¡´ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ í•¨ìˆ˜ (Celery ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ)
async def process_batch_download(batch_id: str):
    """ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)"""
    batch = DownloadBatch.model_validate_json(redis_client.get(f"batch:{batch_id}"))
    if not batch:
        logger.error(f"ë°°ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {batch_id}")
        return
    
    logger.info(f"ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {batch_id}")
    batch.status = "downloading"
    
    # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€ (ìˆœì°¨ ì²˜ë¦¬)
    # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì´ ë°©ì‹ë³´ë‹¤ Celeryë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
    
    from ..core.config import settings
    from ..services.rapidgator_client import RapidgatorClient
    
    try:
        # ë¡œê·¸ì¸ í™•ì¸
        client = RapidgatorClient()
        logged_in = await client.ensure_logged_in()
        if not logged_in:
            logger.error("Rapidgator ë¡œê·¸ì¸ ì‹¤íŒ¨")
            batch.status = "failed"
            for item in batch.items:
                item.status = "failed"
                item.error_message = "ë¡œê·¸ì¸ ì‹¤íŒ¨"
            return
        
        # ê° ì•„ì´í…œ ë‹¤ìš´ë¡œë“œ
        for item in batch.items:
            try:
                item.status = "downloading"
                
                # ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                result = await client.download_file(item.url, settings.download_path)
                
                if result.get("success", False):
                    item.status = "completed"
                    item.filename = result.get("filename")
                    item.file_size = result.get("file_size", 0)
                    item.progress = 100.0
                    batch.completed_items += 1
                else:
                    item.status = "failed"
                    item.error_message = result.get("error", "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                    batch.failed_items += 1
                    
            except Exception as e:
                logger.error(f"ì•„ì´í…œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {item.id} - {str(e)}")
                item.status = "failed"
                item.error_message = str(e)
                batch.failed_items += 1
        
        # ë°°ì¹˜ ì™„ë£Œ ìƒíƒœ ê²°ì •
        if batch.failed_items == 0:
            batch.status = "completed"
        elif batch.completed_items == 0:
            batch.status = "failed"
        else:
            batch.status = "completed_with_errors"
        
        batch.completed_at = time.time()
        
        logger.info(f"ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {batch_id} - {batch.completed_items}ê°œ ì„±ê³µ, {batch.failed_items}ê°œ ì‹¤íŒ¨")
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {batch_id} - {str(e)}")
        batch.status = "failed"
        for item in batch.items:
            if item.status == "pending":
                item.status = "failed"
                item.error_message = f"ë°°ì¹˜ ì‹¤íŒ¨: {str(e)}" 

@router.get("/debug/redis_info")
async def redis_info():
    """Get Redis server information."""
    return await redis_client.info()


@router.get("/stats")
async def get_download_stats():
    """Get overall download statistics."""
    batches = list_all_batches() # ëª¨ë“  ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •
    total_downloads = sum(b.get("total_items", 0) for b in batches)
    completed_downloads = sum(b.get("completed_items", 0) for b in batches)
    
    # Calculate total size (sum of file_size for all completed items)
    total_size_bytes = 0
    for b in batches:
        for item in b.get("items", []):
            if item.get("status") == "completed" and item.get("file_size"):
                total_size_bytes += item.get("file_size", 0)

    # Helper for human-readable size
    def human_readable_size(size, decimal_places=2):
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size < 1024.0:
                break
            size /= 1024.0
        return f"{size:.{decimal_places}f} {unit}"

    return {
        "total_downloads": total_downloads,
        "completed_downloads": completed_downloads,
        "active_downloads": sum(1 for b in batches if b.get("status") in ["downloading", "pending"]),
        "total_size_completed": human_readable_size(total_size_bytes)
    }


@router.get("/recent")
async def get_recent_downloads():
    """Get a list of recently completed download batches."""
    batches_data = list_all_batches()
    all_items = []
    for batch in batches_data:
        for item in batch.get("items", []):
            if item.get("status") == "completed":
                # Add batch name to each item for context
                item['batch_name'] = batch.get('name', 'Untitled Batch')
                all_items.append(item)

    # Sort by completion time, most recent first
    all_items.sort(key=lambda x: x.get("completed_at", 0), reverse=True)
    return all_items[:10] # Return latest 10 completed items 